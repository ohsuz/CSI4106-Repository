{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5 - Natural Language Processing\n",
    "### Exploring the NLP pipeline in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4106 Artificial Intelligence  \n",
    "Fall 2019  \n",
    "Prepared by Caroline BarriÃ¨re and Julian Templeton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***INTRODUCTION***:  \n",
    "\n",
    "This notebook is split in two parts.  In **Part A**, you will explore the different steps of the NLP pipeline, and in **Part B**, you will revisit some of the Machine Learning algorithms used in prior notebooks to perform Polarity Detection on Rotten Tomatoe reviews with NLP techniques used on the reviews.  We will work with the package *nltk* which is very useful for NLP analysis.  You will need to install it before you start.  Information about NLTK are here: http://www.nltk.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HOMEWORK***:  \n",
    "Go through the notebook by running each cell, one at a time.  \n",
    "Look for **(TO DO)** for the tasks that you need to perform. Do not edit the code outside of the questions which you are asked to answer unless specifically asked. Once you're done, Sign the notebook (at the end of the notebook), and submit it.  \n",
    "\n",
    "*The notebook will be marked on 42.  \n",
    "Each **(TO DO)** has a number of points associated with it.*\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install nltk library: pip install nltk\n",
    "# Next import nltk into python\n",
    "import nltk \n",
    "# Open the installer to download nltk packages\n",
    "# This will take a moment to open. Just download all to get eevrything that would need.\n",
    "# Once downloaded you can comment this function to avoid calling when running\n",
    "#nltk.download() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART A - NATURAL LANGUAGE PROCESSING PIPELINE**  \n",
    "  \n",
    "In this part, we will use the modules from *nltk* to perform the different steps of the pipeline.  \n",
    "We first define a small sample text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleText = \"Every week we have two A.I. lectures in this course. The course number is CSI4106. \"\\\n",
    "             \"In the course, we do many Jupyter Notebooks. Each notebook covers a different topic such as a SVM \"\\\n",
    "             \"classifier, Simulated Annealing, Depth-First Search, and many more. In class we have looked \"\\\n",
    "             \"at intricate algorithms, such as the ARC-3 algorithm and neural network learning algorithms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sampleText)\n",
    "# number of tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every', 'week', 'we', 'have', 'two', 'A.I', '.', 'lectures', 'in', 'this', 'course', '.', 'The', 'course', 'number', 'is', 'CSI4106', '.', 'In', 'the', 'course', ',', 'we', 'do', 'many', 'Jupyter', 'Notebooks', '.', 'Each', 'notebook', 'covers', 'a', 'different', 'topic', 'such', 'as', 'a', 'SVM', 'classifier', ',', 'Simulated', 'Annealing', ',', 'Depth-First', 'Search', ',', 'and', 'many', 'more', '.', 'In', 'class', 'we', 'have', 'looked', 'at', 'intricate', 'algorithms', ',', 'such', 'as', 'the', 'ARC-3', 'algorithm', 'and', 'neural', 'network', 'learning', 'algorithms', '.']\n"
     ]
    }
   ],
   "source": [
    "# Showing the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a - Stemming (Porter Stemmer)\n",
    "For a reference to the [algorithm](http://snowballstem.org/algorithms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everi', 'week', 'we', 'have', 'two', 'a.i', '.', 'lectur', 'in', 'thi', 'cours', '.', 'the', 'cours', 'number', 'is', 'csi4106', '.', 'In', 'the', 'cours', ',', 'we', 'do', 'mani', 'jupyt', 'notebook', '.', 'each', 'notebook', 'cover', 'a', 'differ', 'topic', 'such', 'as', 'a', 'svm', 'classifi', ',', 'simul', 'anneal', ',', 'depth-first', 'search', ',', 'and', 'mani', 'more', '.', 'In', 'class', 'we', 'have', 'look', 'at', 'intric', 'algorithm', ',', 'such', 'as', 'the', 'arc-3', 'algorithm', 'and', 'neural', 'network', 'learn', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk contains different stemmers, and we try the Porter Stemmer here\n",
    "from nltk.stem import *\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(t) for t in tokens]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b - Lemmatization\n",
    "The lemmatization relies on a resource called Wordnet (https://wordnet.princeton.edu/), in which lemmas are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lushs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the wordnet resource\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every', 'week', 'we', 'have', 'two', 'A.I', '.', 'lecture', 'in', 'this', 'course', '.', 'The', 'course', 'number', 'is', 'CSI4106', '.', 'In', 'the', 'course', ',', 'we', 'do', 'many', 'Jupyter', 'Notebooks', '.', 'Each', 'notebook', 'cover', 'a', 'different', 'topic', 'such', 'a', 'a', 'SVM', 'classifier', ',', 'Simulated', 'Annealing', ',', 'Depth-First', 'Search', ',', 'and', 'many', 'more', '.', 'In', 'class', 'we', 'have', 'looked', 'at', 'intricate', 'algorithm', ',', 'such', 'a', 'the', 'ARC-3', 'algorithm', 'and', 'neural', 'network', 'learning', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmas = [wnl.lemmatize(t) for t in tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q1 - 2 marks**    \n",
    "Describe in your own words the difference between lemmatization and stemming.  Use examples from above to show the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatization takes context into account, so the results preserve the POS information for that word.\n",
    "However, the results of stemming do not preserve the POS information.\n",
    "In other words, the results of stemming are often words that do not exist in the dictionary.\n",
    "EX) lemmatization: lecture, course -> lecture, course (still noun, preserving its POS)\n",
    "    stemming: lecture, course -> lectur, cours (not preserving its POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Part-Of-Speech tagging  (POS tagging)\n",
    "As we've seen in class, sentence splitting can be learned through a supervised model.  POS tagging can also be learned through a supervised model.  Here, we will use a perceptron model pre-trained in NLTK.  Look here http://www.nltk.org/_modules/nltk/tag/perceptron.html to understand the model.  \n",
    "  \n",
    "The full sest of tags is available [here](https://www.clips.uantwerpen.be/pages/mbsp-tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Every', 'DT'), ('week', 'NN'), ('we', 'PRP'), ('have', 'VBP'), ('two', 'CD'), ('A.I', 'NNP'), ('.', '.'), ('lectures', 'VBZ'), ('in', 'IN'), ('this', 'DT'), ('course', 'NN'), ('.', '.'), ('The', 'DT'), ('course', 'NN'), ('number', 'NN'), ('is', 'VBZ'), ('CSI4106', 'NNP'), ('.', '.'), ('In', 'IN'), ('the', 'DT'), ('course', 'NN'), (',', ','), ('we', 'PRP'), ('do', 'VBP'), ('many', 'JJ'), ('Jupyter', 'NNP'), ('Notebooks', 'NNP'), ('.', '.'), ('Each', 'DT'), ('notebook', 'NN'), ('covers', 'VBZ'), ('a', 'DT'), ('different', 'JJ'), ('topic', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('a', 'DT'), ('SVM', 'NNP'), ('classifier', 'NN'), (',', ','), ('Simulated', 'NNP'), ('Annealing', 'NNP'), (',', ','), ('Depth-First', 'NNP'), ('Search', 'NNP'), (',', ','), ('and', 'CC'), ('many', 'JJ'), ('more', 'JJR'), ('.', '.'), ('In', 'IN'), ('class', 'NN'), ('we', 'PRP'), ('have', 'VBP'), ('looked', 'VBN'), ('at', 'IN'), ('intricate', 'JJ'), ('algorithms', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('the', 'DT'), ('ARC-3', 'NNP'), ('algorithm', 'NN'), ('and', 'CC'), ('neural', 'JJ'), ('network', 'NN'), ('learning', 'VBG'), ('algorithms', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lushs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk contains a method to obtain the part-of-speech of each token\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "posTokens = nltk.pos_tag(tokens)\n",
    "print(posTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('week', 'NN')\n",
      "NN\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "# If we just want to see one tag in particular\n",
    "print(posTokens[1])  # It's a tuple\n",
    "print(posTokens[1][1])  # Second part of the tuple is the tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to Step 2 \n",
    "\n",
    "The lemmatizer we use is based on WordNet (a lexical resource commonly used in NLP) to provide a set of lemmas.  As many words are ambiguous and can be found in sentences as verbs or nouns (remember examples such as *Will's will will be achieved*), the lemmatizer can benefit from knowledge of POS.  Small problem... POS tags in Wordnet are not the same as in Treebank.  Wordnet defines only 4 POS: N (noun), V (verb), J (adjective) and R (adverb). The small method below is to obtain a partial equivalence between the tagsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Try to lemmatize, this time knowing the POS\n",
    "# Tagsets are often different... here we map the treebank tagset (default in pos_tag) \n",
    "# to the wordnet tagset \n",
    "# We will learn more about wordnet when we discuss resources in the Knowledge Representation module of this course\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.ADV  # just use as default, for ADV the lemmatizer doesn't change anything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 'n', 'r', 'v', 'r', 'n', 'r', 'v', 'r', 'r', 'n', 'r', 'r', 'n', 'n', 'v', 'n', 'r', 'r', 'r', 'n', 'r', 'r', 'v', 'a', 'n', 'n', 'r', 'r', 'n', 'v', 'r', 'a', 'n', 'a', 'r', 'r', 'n', 'n', 'r', 'n', 'n', 'r', 'n', 'n', 'r', 'r', 'a', 'a', 'r', 'r', 'n', 'r', 'v', 'v', 'r', 'a', 'n', 'r', 'a', 'r', 'r', 'n', 'n', 'r', 'a', 'n', 'v', 'n', 'r']\n"
     ]
    }
   ],
   "source": [
    "# Transform the tags into wordnet tags\n",
    "wordnet_tags = [get_wordnet_pos(p[1]) for p in posTokens]\n",
    "print(wordnet_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every', 'week', 'we', 'have', 'two', 'A.I', '.', 'lecture', 'in', 'this', 'course', '.', 'The', 'course', 'number', 'be', 'CSI4106', '.', 'In', 'the', 'course', ',', 'we', 'do', 'many', 'Jupyter', 'Notebooks', '.', 'Each', 'notebook', 'cover', 'a', 'different', 'topic', 'such', 'as', 'a', 'SVM', 'classifier', ',', 'Simulated', 'Annealing', ',', 'Depth-First', 'Search', ',', 'and', 'many', 'more', '.', 'In', 'class', 'we', 'have', 'look', 'at', 'intricate', 'algorithm', ',', 'such', 'as', 'the', 'ARC-3', 'algorithm', 'and', 'neural', 'network', 'learn', 'algorithm', '.']\n"
     ]
    }
   ],
   "source": [
    "# Now, let's try to lemmatize again, but we tell the lemmatizer what the POS is.\n",
    "posLemmas = [wnl.lemmatize(t, w) for t, w in zip(tokens, wordnet_tags)]\n",
    "print(posLemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q2 - 1 mark**    \n",
    "Which words are lemmatized differently when provided with the additional knowledge of POS? Look at the variables *lemmas* and *posLemmas* to get this answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is -> be\n",
      "a -> as\n",
      "looked -> look\n",
      "a -> as\n",
      "learning -> learn\n"
     ]
    }
   ],
   "source": [
    "#Q2 - ANSWER   \n",
    "#Format as: <lemmatized_word> -> <POS_lemmatized_word> only for tokens that are different between posLemmas and lemmas.  \n",
    "\n",
    "for i in range(len(lemmas)):\n",
    "    if lemmas[i] != posLemmas[i]:\n",
    "        print(lemmas[i]+\" -> \"+posLemmas[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Every week we have two A.I.', 'lectures in this course.', 'The course number is CSI4106.', 'In the course, we do many Jupyter Notebooks.', 'Each notebook covers a different topic such as a SVM classifier, Simulated Annealing, Depth-First Search, and many more.', 'In class we have looked at intricate algorithms, such as the ARC-3 algorithm and neural network learning algorithms.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence splitting can be done before tokenizing and POS tagging if we wish\n",
    "sentences = nltk.sent_tokenize(sampleText)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q3 - 2 marks**    \n",
    "How many sentences are generated?   \n",
    "Give an explanation for the number of sentences returned from sentence splitting (more, less, the same? Why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThe real number of sentences are less than results from sentence splitting.\\nThis is because sentence splitting is not accurate regarding the end of sentences, so it often breaks sentences even where it is not the end of a sentence.\\n\\nex) Every week we have two A.I. -> this is not the end of the sentence but sentence splitting perceives as the end because of '.'\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3 - ANSWER   \n",
    "#How many sentences were generated?   \n",
    "\n",
    "print(\"The number of sentences: \"+str(len(sentences)))\n",
    "\n",
    "#Give an explanation for the number of sentences returned from sentence splitting (more, less, the same? Why?).  \n",
    "\n",
    "'''\n",
    "The real number of sentences are less than results from sentence splitting.\n",
    "This is because sentence splitting is not accurate regarding the end of sentences, so it often breaks sentences even where it is not the end of a sentence.\n",
    "\n",
    "ex) Every week we have two A.I. -> this is not the end of the sentence but sentence splitting perceives as the end because of '.'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together in a complete example**  \n",
    "Having covered the NLP pipeline, we will now perform these steps on a movie review from the Rotten Tomatoes Polarity Detection dataset that we used in Notebook 3.  \n",
    "  \n",
    "Recall from Notebook 3:   \n",
    "You will need to download the movie review dataset from the following shared Google Drive (or just copy it over from your notebook 3 folder!):\n",
    "https://drive.google.com/file/d/1w1TsJB-gmIkZ28d1j7sf1sqcPmHXw352/view\n",
    "\n",
    "This is a dataset of reviews from Rotten Tomatoes along with the Freshness of the review (Fresh or Rotten). We will be using this dataset throughout the notebook so be sure to place it in the same directory as this notebook. It contains 480000 reviews with half of them being rotten and the other half being fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and load the dataset\n",
    "import pandas as pd\n",
    "# Import the dataset, need to use the ISO-8859-1 encoding due to some invalid UTF-8 characters\n",
    "df = pd.read_csv(\"rt_reviews.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I can't figure out exactly when the film 10,000 B.C. is set, but it's definitely ancient times. Like before they had cars, guns or tabloid blogs. And definitely before they had cohesive plots or dialogue that made sense.\n"
     ]
    }
   ],
   "source": [
    "# We extract a specific review to be used for the remainder of this subsection\n",
    "pipeline_review = df.iloc[11992][1]\n",
    "print(pipeline_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q4 - 5 marks**    \n",
    "Perform all of the steps that we have worked with so far from the NLP Pipeline on the review pipeline_review. You will need to do all of the following:    \n",
    "1. Tokenization\n",
    "2. Lemmatization and Stemming\n",
    "3. POS Tagging\n",
    "4. POS-based lemmatization\n",
    "5. Sentence Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Tokenization\n",
      "\n",
      "['I', 'ca', \"n't\", 'figure', 'out', 'exactly', 'when', 'the', 'film', '10,000', 'B.C', '.', 'is', 'set', ',', 'but', 'it', \"'s\", 'definitely', 'ancient', 'times', '.', 'Like', 'before', 'they', 'had', 'cars', ',', 'guns', 'or', 'tabloid', 'blogs', '.', 'And', 'definitely', 'before', 'they', 'had', 'cohesive', 'plots', 'or', 'dialogue', 'that', 'made', 'sense', '.']\n",
      "\n",
      "\n",
      "After Lemmatization\n",
      "\n",
      "['I', 'ca', \"n't\", 'figure', 'out', 'exactly', 'when', 'the', 'film', '10,000', 'B.C', '.', 'is', 'set', ',', 'but', 'it', \"'s\", 'definitely', 'ancient', 'time', '.', 'Like', 'before', 'they', 'had', 'car', ',', 'gun', 'or', 'tabloid', 'blog', '.', 'And', 'definitely', 'before', 'they', 'had', 'cohesive', 'plot', 'or', 'dialogue', 'that', 'made', 'sense', '.']\n",
      "\n",
      "\n",
      "After Stemming\n",
      "\n",
      "['I', 'ca', \"n't\", 'figur', 'out', 'exactli', 'when', 'the', 'film', '10,000', 'b.c', '.', 'is', 'set', ',', 'but', 'it', \"'s\", 'definit', 'ancient', 'time', '.', 'like', 'befor', 'they', 'had', 'car', ',', 'gun', 'or', 'tabloid', 'blog', '.', 'and', 'definit', 'befor', 'they', 'had', 'cohes', 'plot', 'or', 'dialogu', 'that', 'made', 'sens', '.']\n",
      "\n",
      "\n",
      "After POS Tagging\n",
      "\n",
      "['r', 'r', 'r', 'v', 'r', 'r', 'r', 'r', 'n', 'r', 'n', 'r', 'v', 'v', 'r', 'r', 'r', 'v', 'r', 'a', 'n', 'r', 'r', 'r', 'r', 'v', 'n', 'r', 'n', 'r', 'a', 'n', 'r', 'r', 'r', 'r', 'r', 'v', 'a', 'n', 'r', 'n', 'r', 'v', 'n', 'r']\n",
      "\n",
      "\n",
      "After POS-based lemmatization\n",
      "\n",
      "['I', 'ca', \"n't\", 'figure', 'out', 'exactly', 'when', 'the', 'film', '10,000', 'B.C', '.', 'is', 'set', ',', 'but', 'it', \"'s\", 'definitely', 'ancient', 'time', '.', 'Like', 'before', 'they', 'had', 'car', ',', 'guns', 'or', 'tabloid', 'blogs', '.', 'And', 'definitely', 'before', 'they', 'had', 'cohesive', 'plots', 'or', 'dialogue', 'that', 'made', 'sense', '.']\n",
      "\n",
      "\n",
      "After Sentence Splitting\n",
      "\n",
      "[\" I can't figure out exactly when the film 10,000 B.C.\", \"is set, but it's definitely ancient times.\", 'Like before they had cars, guns or tabloid blogs.', 'And definitely before they had cohesive plots or dialogue that made sense.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO - Q4\n",
    "\n",
    "# Tokenization\n",
    "new_token = word_tokenize(pipeline_review)\n",
    "print(\"After Tokenization\\n\")\n",
    "print(new_token)\n",
    "print(\"\\n\")\n",
    "# Lemmatization\n",
    "new_wnl = nltk.WordNetLemmatizer()\n",
    "new_lemma = [new_wnl.lemmatize(t) for t in new_token]\n",
    "print(\"After Lemmatization\\n\")\n",
    "print(new_lemma)\n",
    "print(\"\\n\")\n",
    "# Stemming\n",
    "new_stemmer = PorterStemmer()\n",
    "new_stem = [new_stemmer.stem(t) for t in new_token]\n",
    "print(\"After Stemming\\n\")\n",
    "print(new_stem)\n",
    "print(\"\\n\")\n",
    "# POS Tagging\n",
    "new_pt = nltk.pos_tag(new_token)\n",
    "new_pos = [get_wordnet_pos(p[1]) for p in new_pt]\n",
    "print(\"After POS Tagging\\n\")\n",
    "print(new_pos)\n",
    "print(\"\\n\")\n",
    "# POS-based lemmatization (Wornet tags then ...)\n",
    "new_pl = [new_wnl.lemmatize(t,w) for t,w in zip(new_token,wordnet_tags)]\n",
    "print(\"After POS-based lemmatization\\n\")\n",
    "print(new_pl)\n",
    "print(\"\\n\")\n",
    "# Sentence Splitting\n",
    "new_ss = nltk.sent_tokenize(pipeline_review)\n",
    "print(\"After Sentence Splitting\\n\")\n",
    "print(new_ss)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "attachments": {
    "CSI4106_Notebook5_corenlp.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 - Parsing\n",
    "The parser in NLTK actually is a wrapper around a java parser (from Stanford NLP).  It's a bit complex to install, so instead, we will use the following online Stanford parse tool to test pipeline_review: http://corenlp.run/   \n",
    "\n",
    "You will need to copy the review from pipeline_review (printed above) into the *Text to annotate* box, select only *parts-of-speech* and *constituency parse* in the *Annotations* box, and click the *Submit* button to generate the output that you will be looking at.  \n",
    "\n",
    "Below is an image exhbiting what was mentioned above (if not visible for some reason, the image is included in the Notebook 5 tab on Brightspace):\n",
    "\n",
    "![CSI4106_Notebook5_corenlp.PNG](attachment:CSI4106_Notebook5_corenlp.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q5 - 2 marks**    \n",
    "Submit the input as described above into the corenlp Web-based tool and answer the following questions.\n",
    "1. Part-of-Speech: How many words are tagged as a verb (in this case any tag starting with V)?\n",
    "2. Constituency Parsing: What does this output represent? How many trees are there? What do the leaves of the trees represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5 - ANSWER   \n",
    "\n",
    "1. 7\n",
    "2. This ouput represents that ,in constituency parsing, the sentence is bound over the club like noun phrase, verb phrase, etc.\n",
    "   There are 3 trees.\n",
    "   The leaves of each trees represent that thay are the components of each phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART B - Supervised Learning using the NLP Pipeline**  \n",
    "  \n",
    "In this second part, we will be re-visiting the algorithms used in notebooks 3 and 4 to perform polarity detection on the Rotten Tomatoes dataset. However, this time we will be using various NLP techniques to modify the reviews before using them as input for the algorithms.  \n",
    " \n",
    "This section will be structured in the following way:  \n",
    "1. First we will walk through how to manipulate the data that we will be using for training and testing using various NLP techniques. Then, you will be given several NLP tasks to perform on the reviews that we will use for training and testing later.   \n",
    "    1.1. NOTE: The tasks will be clearly defined with an example, but you will need to come up with a way to perform using what you have been shown through examples in this notebook.\n",
    "2. Using the modified train and test sets from (1), you will select two of the models that we have used for Supervised Machine Learning (either Naive Bayes or SVM + either Logistic Regression or MLPClassifier) to perform Polarity Detection. You will compare the test results obtained from the models that you will invoke.\n",
    "    2.1. NOTE: You are free to re-use code from Notebooks 3 and 4 to help with this task if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup our training and test datasets (same as from notebook 3)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Randomly select 10000 fresh examples from the dataframe\n",
    "dfFresh = df[df[\"Freshness\"] == \"fresh\"].sample(n=10000, random_state=5)\n",
    "# Randomly select 10000 rotten examples from the dataframe\n",
    "dfRotten = df[df[\"Freshness\"] == \"rotten\"].sample(n=10000, random_state=3)\n",
    "# Combine the results to make a small random subset of reviews to use\n",
    "dfPartial = dfFresh.append(dfRotten)\n",
    "\n",
    "# Split the data such that 90% is used for training and 10% is used for testing (separating the review\n",
    "# from the freshness scores that we will use as the labels)\n",
    "# Recall that we do not use this test set when building the model, only the training set\n",
    "# We use the parameter stratify to split the training and testing data equally to create\n",
    "# a balanced dataset\n",
    "train_reviews, test_reviews, train_tags, test_tags = train_test_split(dfPartial[\"Review\"],\n",
    "                                                                      dfPartial[\"Freshness\"],\n",
    "                                                                      test_size=0.1, \n",
    "                                                                      random_state=10,\n",
    "                                                                      stratify=dfPartial[\"Freshness\"])\n",
    "\n",
    "# Note that we do not convert to numpy arrays here. You can still loop through the contents as normal,\n",
    "# you will just need to use the df.iloc[index] method to get a single review at an index (you will see\n",
    "# that you do not even need to do this for this notebook. Just iterating though the dataframe is all you need!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253536     You Again poses an interesting question -- wh...\n",
      "252809     Joe Swanberg's starriest picture is a lovely ...\n",
      "386907     There is very little here to disabuse the gro...\n",
      "222829     A sensitive portrait, but often a wretched on...\n",
      "418760     Nothing really happens besides self-introspec...\n",
      "335310     The supernatural elements brush up against so...\n",
      "276254     An exciting film that will appeal to all fans...\n",
      "431837     Von Dormael's mischievous sense of humour is ...\n",
      "29935                  full review at Movies for the Masses\n",
      "315929     Surprisingly morbid, with a severe lack of wo...\n",
      "320138     Fulci's signature grand-gugnal gore drips in ...\n",
      "39462      Call this an extreme rom-com... finely crafte...\n",
      "222759     The stone-faced, determined Doupe and the cha...\n",
      "352169     Writer-director Joseph Cedar's understanding ...\n",
      "223740     Director Antoine Fuqua (\"Training Day,\" \"The ...\n",
      "180224     The first movie felt a little stiff, as thoug...\n",
      "404687     The most enjoyable thriller I've seen in year...\n",
      "354601     Marillier is a great find. But French-Belgian...\n",
      "100552     Tidy psychological thriller taut with tension...\n",
      "151393       A pleasant lightweight piece of entertainment.\n",
      "476897     Bruce McDonald's latest is a genre-freak-only...\n",
      "8546       Fuqua has a fairly conventional script for a ...\n",
      "384253     Setting aside so much zombie fiction's dull r...\n",
      "322869                    You'll O.D. on your own laughter!\n",
      "304586     I had a lot of trouble trying to figure out w...\n",
      "109846     This is one of the worst movies of 2017. And ...\n",
      "406644     Seeing The Happening is like going on an outi...\n",
      "111412       Like the Hangover movies, only with nice guys.\n",
      "182201     How to Talk to Girls at Parties is for all th...\n",
      "42732      Mali Blues should appeal both to world music ...\n",
      "                                ...                        \n",
      "273658     ...an intelligent, complex movie about artifi...\n",
      "113785         [An] extraordinary kaleidoscope of a film...\n",
      "459517     [Happy Death Day 2U] is a genre-defying thril...\n",
      "253843     Cleverness is abandoned for stereotypical bur...\n",
      "424522     Despite a finale that fails to live up to the...\n",
      "446437     David Gordon Green's latest film has the amus...\n",
      "16920      I was never engaged in the film's story nor d...\n",
      "285218      The one man demolition squad blasts away again!\n",
      "200856     It's a toss-up between Kleenex and the sick b...\n",
      "388721              Machete runs out of steam pretty early.\n",
      "466382     In Between Days is a small slice of a suspend...\n",
      "413651     While there isn't anything in M:I-2 as memora...\n",
      "416485     even when it's slumming in sentimentality, it...\n",
      "425173     Funny that Owen Wilson should star in a film ...\n",
      "453080     In 70 short minutes, directors Dennis Scholl ...\n",
      "5546       Unrelenting realism ends up tarnishing the fi...\n",
      "148311     ... it's great to see a director and his crew...\n",
      "163544     So, Ridley Scott, when your film asked me, in...\n",
      "145554                Amusing but ultimately disappointing.\n",
      "305343     I kept pulling away from the whole thing, adm...\n",
      "315846     Adamson claims that this was a labor of love,...\n",
      "199947     Unrelentingly anti-Semitic, excessively grote...\n",
      "388605     A more interesting movie if you have some rum...\n",
      "295774                          Profound, this film is not.\n",
      "450020     Aggressively bland, a display of X-treme mild...\n",
      "395461     A two-hour roller-coaster trip, Jurassic Worl...\n",
      "308329     Mr. NoÃ¯Â¿Â½Ã¯Â¿Â½'s juxtaposition of lofty religio...\n",
      "54446      Ira Sachs is a director of uncommon empathy a...\n",
      "40587      Tackling this life with honesty and respect d...\n",
      "43566      It is a film that keeps the facts and scarcel...\n",
      "Name: Review, Length: 18000, dtype: object \n",
      "\n",
      " You Again poses an interesting question -- what if our long-ago bullies were just as psychically scarred by the tormenting as their tormented victims were? -- but that curveball is buried under a lot of gunk.\n"
     ]
    }
   ],
   "source": [
    "# Reminder that the data is a pandas dataframe which you can manipulate easily\n",
    "# You can iterate through it normally and can replace the content at an index\n",
    "# Example of iterating through a dataframe:\n",
    "# for review in train_reviews: \n",
    "#     print(review)\n",
    "# This will loop through the reviews in train_reviews just like a list\n",
    "\n",
    "# Print train_reviews (dataframe of reviews)\n",
    "print(train_reviews, \"\\n\")\n",
    "# Print the first train review\n",
    "print(train_reviews.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Manipulating the movie reviews**   \n",
    "The first task will be to give an example on how to modify the movie reviews that we will be using to perform Polarity Detection. Our goal for this step will be to introduce the idea of *stopword removal* and how to perform stopword removal on every movie review that will be used for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.1. Performing Stopword Removal***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leading into the idea of stopword removal, let us first exhibit how to edit a review with respect to some task. \n",
    "\n",
    "To do this we will go through an example that removes all non-alphanumeric characters in a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  You Again poses an interesting question -- what if our long-ago bullies were just as psychically scarred by the tormenting as their tormented victims were? -- but that curveball is buried under a lot of gunk. \n",
      "\n",
      "Look at our tokenized review:\n",
      "['you', 'again', 'poses', 'an', 'interesting', 'question', '--', 'what', 'if', 'our', 'long-ago', 'bullies', 'were', 'just', 'as', 'psychically', 'scarred', 'by', 'the', 'tormenting', 'as', 'their', 'tormented', 'victims', 'were', '?', '--', 'but', 'that', 'curveball', 'is', 'buried', 'under', 'a', 'lot', 'of', 'gunk', '.'] \n",
      "\n",
      "Look at our alphanumeric tokenized review:\n",
      "['you', 'again', 'poses', 'an', 'interesting', 'question', 'what', 'if', 'our', 'bullies', 'were', 'just', 'as', 'psychically', 'scarred', 'by', 'the', 'tormenting', 'as', 'their', 'tormented', 'victims', 'were', 'but', 'that', 'curveball', 'is', 'buried', 'under', 'a', 'lot', 'of', 'gunk'] \n",
      "\n",
      "Look at our alphanumeric review:\n",
      "you again poses an interesting question what if our bullies were just as psychically scarred by the tormenting as their tormented victims were but that curveball is buried under a lot of gunk\n"
     ]
    }
   ],
   "source": [
    "# Import the Regular expressions library which is used for this example\n",
    "import re\n",
    "\n",
    "# We will use the first review from our training set as an example.\n",
    "ex_review = train_reviews.iloc[0]\n",
    "print(\"Review:\", ex_review, \"\\n\")\n",
    "\n",
    "# We need to tokenize the review first\n",
    "# Recall that we could do this to all reviews in the dataframe by iterating through its\n",
    "# contents the same way as we would a list (for review in dataframe)\n",
    "ex_review_tok = word_tokenize(ex_review.lower())\n",
    "print(\"Look at our tokenized review:\")\n",
    "print(ex_review_tok, \"\\n\")\n",
    "\n",
    "# We then loop through the tokens, keeping only the alphanumeric tokens\n",
    "ex_review_tok_alpha = [t for t in ex_review_tok if re.match(\"^[a-zA-Z]+$\", t)]\n",
    "print(\"Look at our alphanumeric tokenized review:\")\n",
    "print(ex_review_tok_alpha, \"\\n\")\n",
    "\n",
    "# Join the tokens to re-form the sentence\n",
    "ex_review_text = \" \".join(ex_review_tok_alpha)\n",
    "print(\"Look at our alphanumeric review:\")\n",
    "print(ex_review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q6 - 2 marks**    \n",
    "In the above example we removed any non-alphanumeric tokens from our selected review and re-constructed the review. If we want to build a modified training set based on these re-constructed reviews we would need to perform this operation on every review and either replace the original review or create a new list of the re-constructed reviews.  \n",
    "\n",
    "Now, we want to build a new version of both the train and the test reviews with any non-alphanumeric tokens and any *stopwords* removed. A stopword is a word that is too common to be deemed useful when training or testing a model. In nltk, we will use the stopwords list available as a guide on what to *remove*. \n",
    "\n",
    "Complete the code below to create a new version of train_reviews and test_reviews where all reviews do not contain any non-alphanumeric tokens or stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"didn't\", \"hadn't\", 'shan', 'in', \"you've\", 'herself', 'that', 'with', 'very', \"mustn't\", 'under', 'himself', 'they', 'their', 'd', 'he', 'by', 'most', 'her', 'of', 'having', \"haven't\", 'were', 'such', 'had', 'ma', 'as', 'my', 'and', 'to', 'themselves', \"she's\", 'don', 'again', \"shouldn't\", 'yours', 'why', 'because', 'aren', 'into', 'does', 'now', 'for', \"wasn't\", 'ain', 'each', 'are', 'wasn', 'what', 'it', 'than', 'just', 've', \"doesn't\", 'your', 'have', 's', 'did', 'only', 'am', 'there', 'haven', 'theirs', 'couldn', 'will', 'shouldn', 'be', 'mustn', 'didn', 'down', 'when', 'them', 'isn', 'who', 'until', 'ourselves', 'other', 'mightn', 'doing', 'after', 'or', 'below', 'has', 'doesn', 'nor', 'more', 'between', 'too', 'been', \"should've\", 'wouldn', 'can', \"you're\", 'him', 'any', \"don't\", 'the', 'up', 'above', 'its', 'before', 'myself', 'she', 'hadn', \"isn't\", 'me', \"won't\", 'further', 'some', \"hasn't\", 'was', 'over', \"you'd\", 'here', 'needn', 'at', 'yourselves', 'our', 'do', 'hers', 'few', 'both', \"shan't\", 'yourself', 'from', \"aren't\", 'an', 'but', 'won', 't', 'how', 'then', 'ours', \"needn't\", 'm', 're', 'against', 'll', 'about', 'on', 'all', 'is', 'hasn', 'being', 'i', 'if', 'no', 'where', 'so', 'you', 'this', 'through', 'weren', \"weren't\", 'once', 'those', 'whom', 'we', 'out', 'not', \"you'll\", 'itself', 'off', \"couldn't\", 'these', 'his', \"it's\", \"wouldn't\", \"that'll\", 'own', \"mightn't\", 'which', 'should', 'during', 'same', 'while', 'a', 'y', 'o'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lushs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Q6 - Importing the stopwords and looking at what they are\n",
    "\n",
    "# Download the stopwords package if you do not already have it\n",
    "nltk.download('stopwords')\n",
    "# Import the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Can uncomment to view what the stopwords look like\n",
    "setStopWords = set(stopwords.words('english'))\n",
    "print(setStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 - TODO\n",
    "\n",
    "# From a given train or test set, returns a new version of the set (as a numpy array)\n",
    "# with any stopwords or non-alphanumeric characters removed\n",
    "def no_alpnum_no_stopword(reviews):\n",
    "    new_reviews = []\n",
    "    for review in reviews:\n",
    "        # Tokenize\n",
    "        review_1 = word_tokenize(review.lower())\n",
    "        # Remove non-alphanumeric characters\n",
    "        review_2 = [t for t in review_1 if re.match(\"^[a-zA-Z]+$\", t)] \n",
    "        # Remove stopwords after removing non-alphanumeric characters\n",
    "        review_3 = [t for t in review_2 if t not in stopwords.words('english')]\n",
    "        # Re-form the tokens\n",
    "        review_fin = \" \".join(review_3)\n",
    "        # Append to new_reviews\n",
    "        new_reviews.append(review_fin)\n",
    "    return np.array(new_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a few minutes so be patient!\n",
    "# New version of the train set (just modifying the reviews, not the tags)\n",
    "train_reviews_sw = no_alpnum_no_stopword(train_reviews)\n",
    "# New version of the test set (just modifying the reviews, not the tags)\n",
    "test_reviews_sw = no_alpnum_no_stopword(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.2. Performing POS Lemmatization***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q7 - 3 marks**    \n",
    "In the above example we removed any non-alphanumeric tokens and stopwords from train_reviews and test_reviews. You will now need to define the function pos_lemmatize_reviews() below. This function will take as input either *train_reviews* or *test_reviews* and will return the numpy array of the *POS Lemmatized* reviews.\n",
    "\n",
    "Everything needed to perform this task has already been done in this notebook, so you only need to use what we have covered in this notebook to answer the question.\n",
    "\n",
    "Complete the code below to create a new version of train_reviews and test_reviews where all reviews have had POS Lemmatization performed on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7 - TODO\n",
    "\n",
    "# From a given train or test set, returns a new version of the set (as a numpy array)\n",
    "# with reviews that have been POS Lemmatized\n",
    "def pos_lemmatize_reviews(reviews):\n",
    "    new_reviews = []\n",
    "    for review in reviews:  \n",
    "        # Tokenize\n",
    "        review_1 = word_tokenize(review.lower())\n",
    "        # Pos-based lemmatization\n",
    "        wnl = nltk.WordNetLemmatizer()\n",
    "        review_2 = [wnl.lemmatize(t,w) for t,w in zip(review_1,wordnet_tags)]\n",
    "        # Re-form the tokens\n",
    "        review_fin = \" \".join(review_2)\n",
    "        # Append to new_reviews\n",
    "        new_reviews.append(review_fin)\n",
    "    return np.array(new_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version of the train set (just modifying the reviews, not the tags)\n",
    "train_reviews_lm = pos_lemmatize_reviews(train_reviews)\n",
    "# New version of the test set (just modifying the reviews, not the tags)\n",
    "test_reviews_lm = pos_lemmatize_reviews(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1.3. Verb-only reviews***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q8 - 3 marks**    \n",
    " You will now need to define the function verb_only_reviews() below. This function will take as input either train_reviews or test_reviews and will return the numpy array of the reviews with each review containing only verbs.\n",
    "\n",
    "Everything needed to perform this task has already been done in this notebook, so you only need to use what we have covered in this notebook to answer the question.\n",
    "\n",
    "Complete the code below to create a new version of train_reviews and test_reviews where all reviews only contain verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8 - TODO\n",
    "\n",
    "# From a given train or test set, returns a new version of the set (as a numpy array)\n",
    "# with reviews that only contain verbs\n",
    "def verb_only_reviews(reviews):\n",
    "    new_reviews = []\n",
    "    for review in reviews:\n",
    "        # Tokenize\n",
    "        review_1 = word_tokenize(review.lower())\n",
    "        # Pos-based lemmatization\n",
    "        review_2 = nltk.pos_tag(review_1)\n",
    "        review_3 = [ p[0] for p in review_2 if p[1][0] == 'V' ]\n",
    "        # Re-form the tokens\n",
    "        review_fin = \" \".join(review_3)\n",
    "        # Append to new_reviews\n",
    "        new_reviews.append(review_fin)\n",
    "    return np.array(new_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version of the train set (just modifying the reviews, not the tags)\n",
    "train_reviews_vb = verb_only_reviews(train_reviews)\n",
    "# New version of the test set (just modifying the reviews, not the tags)\n",
    "test_reviews_vb = verb_only_reviews(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Polarity Detection using augmented train and test sets**   \n",
    "Now that we have three augmented versions of our original train and test sets we will test them out to see how they compare to one another. You will be provided two algorithm choices in each question where you will need to select one of the algorithms and perform polarity detection on the reviews using the chosen algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2.1. Naive Bayes or SVM***    \n",
    "For this subsection you will perform polarity detection on the augmented movie reviews with *either* the Naive Bayes Classifier or the SVM Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q9 - 9 marks**    \n",
    "For this question you will be selecting *either* the Naive Bayes Classifier (MultinomialNB) used in Notebook 3 or the SVM Classifier used in Notebook 3. You may need to refer to Notebook 3 to help answer this question. Your first task is to select the classifier that you would like to use.   \n",
    "\n",
    "*Note*: You are expected to use only one. If you use only more than one only the first will be marked.\n",
    "\n",
    "For each augmented set of the train and test reviews perform the following:\n",
    "\n",
    "0. Recall that the augmented train and test sets are as followed (you need to perform all of the following with each augmented set):   \n",
    "    0.1. (train_reviews_sw, train_tags), (test_reviews_sw, test_tags)  \n",
    "    0.2. (train_reviews_lm, train_tags), (test_reviews_lm, test_tags)  \n",
    "    0.3. (train_reviews_vb, train_tags), (test_reviews_vb, test_tags)  \n",
    "1. Need to use CountVectorizer() to transform each review set (requires a CountVectorizer() for each set to be fit_tansform() with the train set). The train reviews require .fit_transform() to be used with the CountVectorizer, the test reviews require .transform() to be used with the CountVectorizer.\n",
    "2. Using the parameter setup from Notebook 3, setup and train three different classifiers of your selected classifier type (one for each augmented train/test set). Recall that we train by calling model.fit(train_counts, train_tags) ***Be sure to keep each trained model as a unique variable since you will use them later***\n",
    "3. Using the test sets and the models, test your models by predicting the sentiments from the test set and print the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO Q9 - 1\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect_sw = CountVectorizer()\n",
    "train_counts_sw = count_vect_sw.fit_transform(train_reviews_sw)\n",
    "test_counts_sw = count_vect_sw.transform(test_reviews_sw)\n",
    "\n",
    "count_vect_lm = CountVectorizer()\n",
    "train_counts_lm = count_vect_lm.fit_transform(train_reviews_lm)\n",
    "test_counts_lm = count_vect_lm.transform(test_reviews_lm)\n",
    "\n",
    "count_vect_vb = CountVectorizer()\n",
    "train_counts_vb = count_vect_vb.fit_transform(train_reviews_vb)\n",
    "test_counts_vb = count_vect_vb.transform(test_reviews_vb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\julian templeton\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:241: ConvergenceWarning: Solver terminated early (max_iter=3500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# TO DO Q9 - 2\n",
    "from sklearn.naive_bayes import MultinomialNB # For Naive Bayes\n",
    "\n",
    "from sklearn import svm # For the SVM\n",
    "\n",
    "# For Naive Bayes\n",
    "clf_nb_sw = MultinomialNB().fit(train_counts_sw, train_tags)\n",
    "clf_nb_lm = MultinomialNB().fit(train_counts_lm, train_tags)\n",
    "clf_nb_vb = MultinomialNB().fit(train_counts_vb, train_tags)\n",
    "\n",
    "# For SVM\n",
    "clf_svm_sw = svm.SVC(kernel=\"linear\", random_state=0, max_iter=3500).fit(train_counts_sw, train_tags)\n",
    "clf_svm_lm = svm.SVC(kernel=\"linear\", random_state=0, max_iter=3500).fit(train_counts_lm, train_tags)\n",
    "clf_svm_vb = svm.SVC(kernel=\"linear\", random_state=0, max_iter=3500).fit(train_counts_vb, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-alphanumeric token removal and stopword removal result\n",
      "Naive Bayes 0.757\n",
      "SVM 0.6505\n",
      "POS Lemmatization result\n",
      "Naive Bayes 0.756\n",
      "SVM 0.6015\n",
      "Verbs only result\n",
      "Naive Bayes 0.6095\n",
      "SVM 0.519\n"
     ]
    }
   ],
   "source": [
    "# TO DO Q9 - 3\n",
    "\n",
    "# Test each trained model using the respective test set\n",
    "\n",
    "# For Naive Bayes\n",
    "nb_pred_sw = clf_nb_sw.predict(test_counts_sw)\n",
    "nb_pred_lm = clf_nb_lm.predict(test_counts_lm)\n",
    "nb_pred_vb = clf_nb_vb.predict(test_counts_vb)\n",
    "\n",
    "nb_correct_sw = 0\n",
    "for tag, pred in zip(test_tags, nb_pred_sw):\n",
    "    if (tag == pred):\n",
    "        nb_correct_sw += 1\n",
    "\n",
    "nb_correct_lm = 0\n",
    "for tag, pred in zip(test_tags, nb_pred_lm):\n",
    "    if (tag == pred):\n",
    "        nb_correct_lm += 1\n",
    "        \n",
    "nb_correct_vb = 0\n",
    "for tag, pred in zip(test_tags, nb_pred_vb):\n",
    "    if (tag == pred):\n",
    "        nb_correct_vb += 1\n",
    "\n",
    "# For SVM\n",
    "svm_pred_sw = clf_svm_sw.predict(test_counts_sw)\n",
    "svm_pred_lm = clf_svm_lm.predict(test_counts_lm)\n",
    "svm_pred_vb = clf_svm_vb.predict(test_counts_vb)\n",
    "\n",
    "svm_correct_sw = 0\n",
    "for tag, pred in zip(test_tags, svm_pred_sw):\n",
    "    if (tag == pred):\n",
    "        svm_correct_sw += 1\n",
    "\n",
    "svm_correct_lm = 0\n",
    "for tag, pred in zip(test_tags, svm_pred_lm):\n",
    "    if (tag == pred):\n",
    "        svm_correct_lm += 1\n",
    "        \n",
    "svm_correct_vb = 0\n",
    "for tag, pred in zip(test_tags, svm_pred_vb):\n",
    "    if (tag == pred):\n",
    "        svm_correct_vb += 1\n",
    "\n",
    "print(\"Non-alphanumeric token removal and stopword removal result\")\n",
    "print(\"Naive Bayes\", (nb_correct_sw / len(nb_pred_sw)))\n",
    "print(\"SVM\", (svm_correct_sw / len(svm_pred_sw)))\n",
    "print(\"POS Lemmatization result\")\n",
    "print(\"Naive Bayes\", (nb_correct_lm / len(nb_pred_lm)))\n",
    "print(\"SVM\", (svm_correct_lm / len(svm_pred_lm)))\n",
    "print(\"Verbs only result\")\n",
    "print(\"Naive Bayes\", (nb_correct_vb / len(nb_pred_vb)))\n",
    "print(\"SVM\", (svm_correct_vb / len(svm_pred_vb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q10 - 2 marks**    \n",
    "Which NLP technique performed on the reviews gave the best test results above? Why do you think that approach gave the best results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the non-alphanumeric tokens and the stopwords yeilded the best test results. This has to do with removing less relevant words, making it easier to derive the polarity of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2.2. Logistic Regression or MLP***    \n",
    "For this subsection you will perform polarity detection on the augmented movie reviews with *either* the Logistic Regression or a MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q11 - 6 marks**    \n",
    "For this question you will be selecting *either* LogisticRegression as used in Notebook 4 or the MLPClassifier used in Notebook 4. You may need to refer to Notebook 4 to help answer this question. Your first task is to select the classifier that you would like to use (this time the MLP will be faster so do not worry about the computational complexity when choosing). The MLPClassifier will now use smaller hidden layer values, so use the provided structure for all of your models.\n",
    "\n",
    "*Note*: You are expected to use only one. If you use only more than one only the first will be marked.\n",
    "\n",
    "For each augmented set of the train and test reviews perform the following:\n",
    "\n",
    "0. You have already setup the transformed train and test data with the CountVectorizers from Q9. You will be re-using their values to train the model with here (so already complete)\n",
    "1. Setup and train three different classifiers of your selected classifier type (one for each augmented train/test set, for only *one epoch*). Recall that we train these models by calling model.fit(train_counts, train_tags). ***Be sure to keep each trained model as a unique variable since you will use them later***\n",
    "2. Using the test sets and the models, test your models by retrieving the score of the model predicting the test sets after being trained. Print out these scores afterwards. Recall that we get the score by calling ... = model.score(test_counts, test_tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(50, 25), learning_rate='constant',\n",
       "              learning_rate_init=0.01, max_iter=100, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO Q11 - 1\n",
    "\n",
    "# For Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# For LogisticRegression\n",
    "clf_lr_sw = LogisticRegression(solver='lbfgs', multi_class=\"multinomial\", max_iter=1000, random_state=1)\n",
    "clf_lr_sw.fit(train_counts_sw, train_tags)\n",
    "clf_lr_lm = LogisticRegression(solver='lbfgs', multi_class=\"multinomial\", max_iter=1000, random_state=1)\n",
    "clf_lr_lm.fit(train_counts_lm, train_tags)\n",
    "clf_lr_vb = LogisticRegression(solver='lbfgs', multi_class=\"multinomial\", max_iter=1000, random_state=1)\n",
    "clf_lr_vb.fit(train_counts_vb, train_tags)\n",
    "\n",
    "# For MLPClassifier\n",
    "clf_mlp_sw = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(50, 25), random_state=1, max_iter=100, learning_rate_init=0.01, warm_start=True)\n",
    "clf_mlp_sw.fit(train_counts_sw, train_tags)\n",
    "clf_mlp_lm = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(50, 25), random_state=1, max_iter=100, learning_rate_init=0.01, warm_start=True)\n",
    "clf_mlp_lm.fit(train_counts_lm, train_tags)\n",
    "clf_mlp_vb = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(50, 25), random_state=1, max_iter=100, learning_rate_init=0.01, warm_start=True)\n",
    "clf_mlp_vb.fit(train_counts_vb, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-alphanumeric token removal and stopword removal score\n",
      "LinearRegression: 0.7355\n",
      "MLP: 0.7185\n",
      "POS Lemmatization score\n",
      "LinearRegression: 0.749\n",
      "MLP: 0.732\n",
      "Verbs only score\n",
      "LinearRegression: 0.6065\n",
      "MLP: 0.6115\n"
     ]
    }
   ],
   "source": [
    "# TO DO Q11 - 2\n",
    "\n",
    "# Test each trained model using the respective test set\n",
    "\n",
    "# For LogisticRegressor\n",
    "score_lr_sw = clf_lr_sw.score(test_counts_sw, test_tags)\n",
    "score_lr_lm = clf_lr_lm.score(test_counts_lm, test_tags)\n",
    "score_lr_vb = clf_lr_vb.score(test_counts_vb, test_tags)\n",
    "\n",
    "# For LogisticRegressor\n",
    "score_mlp_lm = clf_mlp_lm.score(test_counts_lm, test_tags)\n",
    "score_mlp_vb = clf_mlp_vb.score(test_counts_vb, test_tags)\n",
    "\n",
    "# Print the test accuracies of each model using the predictions that you have obtained above\n",
    "print(\"Non-alphanumeric token removal and stopword removal score\")\n",
    "print(\"LogisticRegression:\", score_lr_sw)\n",
    "print(\"MLP:\", score_mlp_sw)\n",
    "print(\"POS Lemmatization score\")\n",
    "print(\"LogisticRegression:\", score_lr_lm)\n",
    "print(\"MLP:\", score_mlp_lm)\n",
    "print(\"Verbs only score\")\n",
    "print(\"LogisticRegression:\", score_lr_vb)\n",
    "print(\"MLP:\", score_mlp_vb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q12 - 2 marks**    \n",
    "Which NLP technique performed on the reviews gave the best test results above? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q12 - ANSWER   \n",
    "POS Lemmatization gave the best results. The extra tokens not cutoff, but POS lemmatized most likely helped the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature\n",
    "\n",
    "I, SUZIE OH, declare that the answers provided in this notebook are my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
